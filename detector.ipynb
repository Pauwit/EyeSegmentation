{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Papillae Detection",
   "id": "5bee7fb1d0f148b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Description",
   "id": "4bc02f8f707c35ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is a notebook that detects the papillae from M0 images.\\\n",
    "It uses a UResNet CNN model, that has been pretrained accordingly.\\\n",
    "**To use it:**\n",
    "- Change \"MODEL_PATH\" to the path that points to your pretrained model (.pth extension)\n",
    "- Execute every cell until the \"Main\" section\n",
    "- Modify \"IMAGE_PATH\"\n",
    "- If needed, modify \"SAVE_PATH\"\n",
    "- Execute all cells of the \"Main\" section"
   ],
   "id": "5efd1c18dfc5a470"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports and Constants",
   "id": "83cb18101dad122b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODEL_PATH = \"states/optic_disk_10.pth\" # /!\\ Change this to your model path\n",
    "IMAGE_SIZE = 1024 # /!\\ DO NOT CHANGE"
   ],
   "id": "124081d6e62c8173",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "⚠️⚠️⚠️ Image size\\\n",
    "If the image size of M0 pictures changes, do not modify this number.\\\n",
    "Right now, the M0 pictures have the size 1023 x 1023. So, one pixel is padded\\\n",
    "on the bottom and on the right of the picture in the \"eval_image\" function.\\\n",
    "**IF THE M0 SIZE CHANGES**, getting to 1024 x 1024 for example, you can remove the padding in \"eval_image\"."
   ],
   "id": "f21ef27799d581ea"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.measure import label, regionprops\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)          # should show 2.8.0+cu126\n",
    "print(torch.version.cuda)         # should show '12.6'\n",
    "print(torch.cuda.is_available())  # should be True\n",
    "print(torch.cuda.get_device_name(0))  # NVIDIA RTX A6000"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_workers = 1\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(\"Using GPU\")\n",
    "    num_workers = torch.cuda.device_count() * 4\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "generator = torch.Generator(device=device)"
   ],
   "id": "cbf01966dd7d4a0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def free_cache():\n",
    "    \"\"\"\n",
    "    Releases unused CUDA memory\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "6c1da92519b486af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T12:38:28.016496Z",
     "start_time": "2025-09-24T12:38:28.011424Z"
    }
   },
   "cell_type": "markdown",
   "source": "## UResNet",
   "id": "e0779fe9a6782518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        # To match dimensions for the skip connection if in/out channels differ\n",
    "        self.shortcut = (\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = self.conv(x)\n",
    "        out += residual\n",
    "        return self.relu(out)"
   ],
   "id": "5bf71686c3e1076c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class DownResSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownResSample, self).__init__()\n",
    "        self.conv = ResBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        p = self.pool(y)\n",
    "        return y, p"
   ],
   "id": "ed2444c7f122f8f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class UpResSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpResSample, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = ResBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x2 = self.up(x2)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return self.conv(x)"
   ],
   "id": "8acdd1e51d71c8c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class UResNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(UResNet, self).__init__()\n",
    "        self.down_conv1 = DownResSample(in_channels, 64)\n",
    "        self.down_conv2 = DownResSample(64, 128)\n",
    "        self.down_conv3 = DownResSample(128, 256)\n",
    "        self.down_conv4 = DownResSample(256, 512)\n",
    "\n",
    "        self.bottle_neck = ResBlock(512, 1024)\n",
    "\n",
    "        self.up_conv1 = UpResSample(1024, 512)\n",
    "        self.up_conv2 = UpResSample(512, 256)\n",
    "        self.up_conv3 = UpResSample(256, 128)\n",
    "        self.up_conv4 = UpResSample(128, 64)\n",
    "\n",
    "        self.out = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1, down1 = self.down_conv1(x)\n",
    "        s2, down2 = self.down_conv2(down1)\n",
    "        s3, down3 = self.down_conv3(down2)\n",
    "        s4, down4 = self.down_conv4(down3)\n",
    "\n",
    "        b = self.bottle_neck(down4)\n",
    "\n",
    "        up1 = self.up_conv1(s4, b)\n",
    "        up2 = self.up_conv2(s3, up1)\n",
    "        up3 = self.up_conv3(s2, up2)\n",
    "        up4 = self.up_conv4(s1, up3)\n",
    "\n",
    "        return self.out(up4)"
   ],
   "id": "8beb065be2debb93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_model(path: str = MODEL_PATH):\n",
    "    \"\"\"\n",
    "    Loads pretrained model\n",
    "    :param path: Trained model path (.pth extension) (str)\n",
    "    :return: PyTorch model in UResNet format\n",
    "    \"\"\"\n",
    "    m = UResNet(in_channels=1, num_classes=1).to(device)\n",
    "    m.load_state_dict(torch.load(path, map_location=torch.device(device)))\n",
    "    return m"
   ],
   "id": "ed0b9bbe4d131589",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T12:40:42.864452Z",
     "start_time": "2025-09-24T12:40:42.861733Z"
    }
   },
   "cell_type": "markdown",
   "source": "## Connected",
   "id": "d7add8933966c245"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def keep_largest_connected_object(pred_mask: torch.Tensor, threshold: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Keep only the largest connected object from a grayscale prediction tensor\n",
    "    :param pred_mask: torch.Tensor of shape [1,H,W] or [H,W], values in [0,1]\n",
    "    :param threshold: Threshold to binarize prediction\n",
    "    :return: torch.Tensor of shape [1,H,W] with only the largest connected object\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    real_mask = pred_mask.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # Threshold to binary\n",
    "    binary = (real_mask > threshold).astype(np.uint8)\n",
    "\n",
    "    if binary.sum() == 0:\n",
    "        return torch.zeros_like(pred_mask)  # no object detected\n",
    "\n",
    "    # Label connected components\n",
    "    labeled = label(binary)\n",
    "    regions = regionprops(labeled)\n",
    "\n",
    "    if not regions:\n",
    "        return torch.zeros_like(pred_mask)\n",
    "\n",
    "    # Keep only the largest region\n",
    "    largest_region = max(regions, key=lambda r: r.area)\n",
    "    largest_mask = (labeled == largest_region.label)\n",
    "\n",
    "    # Convert back to torch tensor\n",
    "    return torch.from_numpy(largest_mask).unsqueeze(0).to(pred_mask.device).float()"
   ],
   "id": "687a7004c58a5a97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save",
   "id": "871068bbd1a66e23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_mask(m: torch.Tensor, path: str):\n",
    "    \"\"\"\n",
    "    Save a mask tensor as a PNG image\n",
    "    :param m: torch.Tensor of shape [1,H,W] or [H,W], values in [0,1] or [0,255]\n",
    "    :param path: Where to save the PNG (str)\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        return\n",
    "\n",
    "    # Remove batch/channel dims if needed\n",
    "    m = m.squeeze()\n",
    "\n",
    "    # Scale to 0–255 if not already\n",
    "    if m.max() <= 1.0:\n",
    "        m = (m * 255).byte()\n",
    "    else:\n",
    "        m = m.byte()\n",
    "\n",
    "    # Convert to PIL image and save\n",
    "    img = Image.fromarray(m.cpu().numpy())\n",
    "    img.save(path)"
   ],
   "id": "f745974a4455a42e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation and Visualization",
   "id": "f068850714c5a68a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_prediction(img: torch.Tensor, mask: torch.Tensor, title: str = None):\n",
    "    \"\"\"\n",
    "    Visualize an image tensor and its mask side by side with the overlay of both\n",
    "    :param img: torch.Tensor of shape [1,H,W] or [H,W] (grayscale image)\n",
    "    :param mask: torch.Tensor of shape [1,H,W] or [H,W] (binary/float mask)\n",
    "    :param title: Optional string to display as figure title\n",
    "    \"\"\"\n",
    "    # Squeeze batch/channel dims if necessary\n",
    "    img_np = img.squeeze().cpu().numpy()\n",
    "    mask_np = mask.squeeze().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    # Show image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img_np, cmap=\"gray\")\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Show mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask_np, cmap=\"gray\")\n",
    "    plt.title(\"Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Show Overlay\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(img_np, cmap=\"gray\")\n",
    "    plt.imshow(mask_np, cmap=\"Reds\", alpha=0.1)\n",
    "    plt.title(\"Overlay\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    if title is not None:\n",
    "        plt.suptitle(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "21edd9cb91cc859",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def eval_image(model, image_path: str, save_path: str = None):\n",
    "    \"\"\"\n",
    "    Forward pass on a single image and keep only the largest connected object\n",
    "    :param model: Trained PyTorch model\n",
    "    :param image_path: Path to the input M0 image (str)\n",
    "    :param save_path: Path to save mask (str | None)\n",
    "    :return: torch.Tensor of shape [1,H,W] -> the largest connected object mask\n",
    "    \"\"\"\n",
    "    # --- Predict mask ---\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Pad(padding=(0, 0, 1, 1)),  # /!\\ because base image is 1023 x 1023\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Load image\n",
    "    img = Image.open(image_path).convert(\"L\")\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)  # add batch dim\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        pred = model(img_tensor)\n",
    "\n",
    "    # Remove batch dimension -> shape [1,H,W]\n",
    "    pred_mask = pred.squeeze(0)\n",
    "\n",
    "    # Apply the largest object filter\n",
    "    largest_mask = keep_largest_connected_object(pred_mask, threshold=0.5)\n",
    "\n",
    "    # --- Draw ---\n",
    "    visualize_prediction(img_tensor, largest_mask)\n",
    "\n",
    "    # --- Save ---\n",
    "    if save_path is not None:\n",
    "        save_mask(largest_mask, save_path)\n",
    "\n",
    "    return largest_mask"
   ],
   "id": "13ad2e1777932016",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main",
   "id": "1e9107b2195b5c1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trained_model = load_model(MODEL_PATH)",
   "id": "6bf253a6ffcb8c31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# /!\\ The M0 image to scan TODO\n",
    "IMAGE_PATH = \"data/optic_disk/train/0676.png\"\n",
    "\n",
    "# /!\\ The path to save the mask to or None (if you don't want to save it) TODO\n",
    "SAVE_PATH = None"
   ],
   "id": "9639bdc011f5220f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate and save mask\n",
    "eval_image(trained_model, IMAGE_PATH, SAVE_PATH)\n",
    "free_cache()"
   ],
   "id": "25db0c7c97ba38fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bc05dc1c7ea3f6e7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
